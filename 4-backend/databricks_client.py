import os
import time
import requests
import logging

logging.basicConfig(level=logging.INFO)

# ================================
# CONFIGURACI√ìN DATABRICKS (ENV VARS)
# ================================
DATABRICKS_WORKSPACE_URL = os.environ.get("DATABRICKS_WORKSPACE_URL")
DATABRICKS_TOKEN = os.environ.get("DATABRICKS_TOKEN")
CLUSTER_ID = os.environ.get("DATABRICKS_CLUSTER_ID")

if not DATABRICKS_WORKSPACE_URL:
    raise RuntimeError("‚ùå ERROR: La variable DATABRICKS_WORKSPACE_URL no est√° definida en las variables de entorno.")

if not DATABRICKS_TOKEN:
    raise RuntimeError("‚ùå ERROR: La variable DATABRICKS_TOKEN no est√° definida en las variables de entorno.")


HEADERS = {
    "Authorization": f"Bearer {DATABRICKS_TOKEN}",
    "Content-Type": "application/json"
}


# ================================
# NOTEBOOK PATHS EN DATABRICKS
# ================================
NOTEBOOKS = {
    "practitioner": [
        "/Workspace/Users/leogcardenasp@gmail.com/Limpiar Practitioner",
        "/Workspace/Users/leogcardenasp@gmail.com/Transformar Practitioner",
        "/Workspace/Users/leogcardenasp@gmail.com/Vista Practitioner"
    ],
    "continuous_integration": [
        "/Workspace/Users/leogcardenasp@gmail.com/Limpiar Continuous Integration",
        "/Workspace/Users/leogcardenasp@gmail.com/Transformar Continuous Integration",
        "/Workspace/Users/leogcardenasp@gmail.com/Vista Continuous Integration"
    ]
}


# ================================
# Listar clusters disponibles
# ================================
def list_clusters():
    """Lista todos los clusters disponibles para diagn√≥stico"""
    logging.info("üìã Listando clusters disponibles...")
    url = f"{DATABRICKS_WORKSPACE_URL}/api/2.0/clusters/list"
    
    try:
        response = requests.get(url, headers=HEADERS)
        
        if response.status_code == 200:
            clusters = response.json().get("clusters", [])
            if clusters:
                for cluster in clusters:
                    logging.info(
                        f"üìå Cluster: {cluster['cluster_name']} | "
                        f"ID: {cluster['cluster_id']} | "
                        f"Estado: {cluster['state']}"
                    )
            else:
                logging.warning("‚ö†Ô∏è No se encontraron clusters")
            return clusters
        else:
            logging.error(f"‚ùå Error listando clusters: {response.text}")
            return []
    except Exception as e:
        logging.error(f"‚ùå Excepci√≥n listando clusters: {str(e)}")
        return []


# ================================
# Verificar y arrancar cluster si es necesario
# ================================
def ensure_cluster_running(cluster_id: str):
    """Asegura que el cluster est√© en estado RUNNING"""
    logging.info(f"üîç Verificando estado del cluster: {cluster_id}")
    
    url = f"{DATABRICKS_WORKSPACE_URL}/api/2.0/clusters/get"
    
    try:
        response = requests.get(url, headers=HEADERS, params={"cluster_id": cluster_id})
        
        if response.status_code != 200:
            logging.error(f"‚ùå Error obteniendo estado del cluster: {response.text}")
            logging.info("üìã Intentando listar clusters disponibles...")
            list_clusters()
            raise Exception(f"‚ùå No se pudo obtener informaci√≥n del cluster {cluster_id}")
        
        cluster_info = response.json()
        state = cluster_info.get("state")
        cluster_name = cluster_info.get("cluster_name", "Unknown")
        
        logging.info(f"üìä Cluster '{cluster_name}' est√° en estado: {state}")
        
        if state == "TERMINATED":
            logging.info("üîÑ Cluster terminado. Iniciando cluster...")
            start_url = f"{DATABRICKS_WORKSPACE_URL}/api/2.0/clusters/start"
            start_response = requests.post(
                start_url, 
                headers=HEADERS, 
                json={"cluster_id": cluster_id}
            )
            
            if start_response.status_code != 200:
                raise Exception(f"‚ùå Error iniciando cluster: {start_response.text}")
            
            # Esperar a que el cluster inicie
            max_wait = 300  # 5 minutos
            start_time = time.time()
            
            while True:
                if time.time() - start_time > max_wait:
                    raise Exception("‚õî Timeout esperando que el cluster inicie")
                
                response = requests.get(url, headers=HEADERS, params={"cluster_id": cluster_id})
                current_state = response.json().get("state")
                
                logging.info(f"   ‚Üí Estado actual: {current_state}")
                
                if current_state == "RUNNING":
                    logging.info("‚úÖ Cluster iniciado y listo")
                    break
                elif current_state == "ERROR":
                    raise Exception("‚ùå El cluster entr√≥ en estado ERROR")
                
                time.sleep(10)
        
        elif state == "PENDING" or state == "RESTARTING":
            logging.info(f"‚è≥ Cluster en estado {state}. Esperando...")
            max_wait = 300
            start_time = time.time()
            
            while True:
                if time.time() - start_time > max_wait:
                    raise Exception(f"‚õî Timeout esperando que el cluster est√© listo desde estado {state}")
                
                response = requests.get(url, headers=HEADERS, params={"cluster_id": cluster_id})
                current_state = response.json().get("state")
                
                logging.info(f"   ‚Üí Estado actual: {current_state}")
                
                if current_state == "RUNNING":
                    logging.info("‚úÖ Cluster listo")
                    break
                elif current_state == "ERROR":
                    raise Exception("‚ùå El cluster entr√≥ en estado ERROR")
                
                time.sleep(10)
        
        elif state == "RUNNING":
            logging.info("‚úÖ Cluster ya est√° corriendo")
        
        else:
            logging.warning(f"‚ö†Ô∏è Estado desconocido del cluster: {state}")
        
        return True
        
    except Exception as e:
        logging.error(f"‚ùå Error verificando/iniciando cluster: {str(e)}")
        raise


# ================================
# Ejecutar notebook como Job temporal
# ================================
def run_notebook(notebook_path: str, params: dict):
    logging.info(f"‚ñ∂Ô∏è Ejecutando notebook: {notebook_path}")
    logging.info(f"üìù Par√°metros: {params}")
    
    # Verificar que el cluster existe y est√° corriendo
    ensure_cluster_running(CLUSTER_ID)
    
    payload = {
        "run_name": f"Pipeline - {notebook_path.split('/')[-1]}",
        "existing_cluster_id": CLUSTER_ID,
        "notebook_task": {
            "notebook_path": notebook_path,
            "base_parameters": params
        },
        "timeout_seconds": 3600  # 1 hora de timeout
    }
    
    url = f"{DATABRICKS_WORKSPACE_URL}/api/2.1/jobs/runs/submit"
    
    try:
        response = requests.post(url, headers=HEADERS, json=payload)
        
        if response.status_code != 200:
            logging.error(f"‚ùå Response status: {response.status_code}")
            logging.error(f"‚ùå Response body: {response.text}")
            raise Exception(
                f"‚ùå Error lanzando notebook '{notebook_path}': {response.text}"
            )
        
        run_id = response.json()["run_id"]
        logging.info(f"‚úî Notebook lanzado correctamente. Run ID: {run_id}")
        
        return run_id
        
    except Exception as e:
        logging.error(f"‚ùå Excepci√≥n lanzando notebook: {str(e)}")
        raise


# ================================
# Esperar a que un run termine
# ================================
def wait_for_run(run_id: int, timeout: int = 900):
    logging.info(f"‚è≥ Esperando ejecuci√≥n del Run ID: {run_id}")
    
    url = f"{DATABRICKS_WORKSPACE_URL}/api/2.1/jobs/runs/get"
    start = time.time()
    
    while True:
        try:
            response = requests.get(url, headers=HEADERS, params={"run_id": run_id})
            
            if response.status_code != 200:
                logging.error(f"‚ùå Error consultando estado del run: {response.text}")
                time.sleep(5)
                continue
            
            state = response.json().get("state", {})
            life = state.get("life_cycle_state")
            result = state.get("result_state")
            
            logging.info(f"   ‚Üí Estado actual: {life} / {result}")
            
            if life == "TERMINATED":
                if result == "SUCCESS":
                    logging.info("‚úî Notebook finaliz√≥ correctamente.")
                    return True
                else:
                    error_info = state.get("state_message", "Error desconocido")
                    raise Exception(f"‚ùå Notebook fall√≥: {error_info}")
            
            if time.time() - start > timeout:
                raise Exception(f"‚õî Timeout esperando el run {run_id}")
            
            time.sleep(5)
            
        except Exception as e:
            if "Notebook fall√≥" in str(e) or "Timeout" in str(e):
                raise
            logging.error(f"‚ùå Error en wait_for_run: {str(e)}")
            time.sleep(5)


# ================================
# Ejecutar los notebooks secuenciales
# ================================
def trigger_notebook_run(destination: str, adls_path: str):
    """
    Ejecuta la secuencia de notebooks para un destino espec√≠fico
    
    Args:
        destination: 'practitioner' o 'continuous_integration'
        adls_path: Ruta del archivo en ADLS
    """
    
    if destination not in NOTEBOOKS:
        raise ValueError(f"‚ùå Destino inv√°lido: {destination}. Debe ser 'practitioner' o 'continuous_integration'")
    
    params = {"input_path": adls_path}
    notebook_sequence = NOTEBOOKS[destination]
    
    logging.info("=" * 60)
    logging.info(f"üöÄ Pipeline Databricks para: {destination}")
    logging.info(f"üìÑ Archivo ADLS: {adls_path}")
    logging.info(f"üîß Cluster ID: {CLUSTER_ID}")
    logging.info("=" * 60)
    
    try:
        for idx, nb in enumerate(notebook_sequence, 1):
            logging.info(f"\n{'='*60}")
            logging.info(f"üìì Paso {idx}/{len(notebook_sequence)}: {nb.split('/')[-1]}")
            logging.info(f"{'='*60}")
            
            run_id = run_notebook(nb, params)
            wait_for_run(run_id)
        
        logging.info("\n" + "=" * 60)
        logging.info("üèÅ Pipeline completado exitosamente.")
        logging.info("=" * 60)
        return True
        
    except Exception as e:
        logging.error("\n" + "=" * 60)
        logging.error(f"üí• Pipeline fall√≥: {str(e)}")
        logging.error("=" * 60)
        raise


# ================================
# Funci√≥n de diagn√≥stico (opcional)
# ================================
def diagnose_cluster():
    """Funci√≥n auxiliar para diagnosticar problemas con el cluster"""
    logging.info("\nüîç DIAGN√ìSTICO DE CLUSTER")
    logging.info("=" * 60)
    
    logging.info(f"Cluster ID configurado: {CLUSTER_ID}")
    logging.info(f"Workspace URL: {DATABRICKS_WORKSPACE_URL}")
    
    # Listar todos los clusters
    list_clusters()
    
    # Verificar estado del cluster espec√≠fico
    try:
        ensure_cluster_running(CLUSTER_ID)
    except Exception as e:
        logging.error(f"‚ùå No se pudo verificar el cluster: {str(e)}")
    
    logging.info("=" * 60)


# Ejemplo de uso si ejecutas este archivo directamente
if __name__ == "__main__":
    # Descomentar para diagnosticar
    # diagnose_cluster()
    pass